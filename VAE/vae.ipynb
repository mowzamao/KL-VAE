{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 11:36:45.844143: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-14 11:36:45.858480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-14 11:36:45.870782: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-14 11:36:45.874411: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-14 11:36:45.884966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-14 11:36:46.549794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cyclical Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_annealing(epoch, n_cycles, ratio):\n",
    "    cycle_length = np.floor(epoch / n_cycles)\n",
    "    cycle_position = epoch - cycle_length * n_cycles\n",
    "    return np.minimum(1.0, cycle_position / (n_cycles * ratio))\n",
    "\n",
    "\n",
    "class CyclicalAnnealingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_epochs, n_cycles, ratio, **kwargs):\n",
    "        super().__init__()\n",
    "        self.total_epochs = total_epochs\n",
    "        self.n_cycles = n_cycles\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        annealing = cyclical_annealing(epoch, self.n_cycles, self.ratio)\n",
    "        self.model.kl_annealing.assign(annealing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling layer: Implements the reparametrization trick for sampling from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package='custom', name='VAE')\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear block: simplifying implementing batch normalisation and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package='custom', name='VAE')\n",
    "class LinearBlock(keras.Model):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.units = units\n",
    "        self.dense = layers.Dense(units, activation=\"relu\", kernel_initializer='he_normal')\n",
    "        self.batch_norm = layers.BatchNormalization()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.call(tf.random.normal(input_shape))\n",
    "        self.built = True\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"units\": self.units,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense(inputs)\n",
    "        x = self.batch_norm(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package='custom', name='VAE')\n",
    "class Encoder(keras.Model):\n",
    "    \"\"\"Maps input to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=32, masking_ratio=0.5, masking_value=-1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.masking_value = masking_value\n",
    "\n",
    "        self.masking = layers.Masking(mask_value=self.masking_value)\n",
    "        self.layer1 = LinearBlock(2048)\n",
    "        self.layer2 = LinearBlock(1024)\n",
    "        self.layer3 = LinearBlock(512)\n",
    "        self.layer4 = LinearBlock(256)\n",
    "        self.layer5 = LinearBlock(256)\n",
    "\n",
    "        self.dense_mean = layers.Dense(self.latent_dim) \n",
    "        self.dense_log_var = layers.Dense(self.latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def random_masking(self, data, ratio=None, mask_val=None):\n",
    "        \"\"\"Randomly masks input data.\n",
    "        \n",
    "        Args:\n",
    "            data: Input data to be masked.\n",
    "            ratio: The ratio of masking. Defaults to instance's masking_ratio.\n",
    "            mask_val: The value to mask. Defaults to instance's masking_value.\n",
    "        \n",
    "        Returns:\n",
    "            Masked data.\n",
    "        \"\"\"\n",
    "        ratio = ratio or self.masking_ratio\n",
    "        mask_val = mask_val or self.masking_value\n",
    "        mask = tf.random.uniform(shape=tf.shape(data)) > ratio\n",
    "        return tf.where(mask, data, tf.fill(tf.shape(data), mask_val))\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Returns the configuration of the Encoder.\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"masking_ratio\": self.masking_ratio,\n",
    "            \"masking_value\": self.masking_value,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build the model by calling it with a random input.\"\"\"\n",
    "        super().build(input_shape)\n",
    "        self.call(tf.random.normal(input_shape))\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Forward pass for the Encoder.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor.\n",
    "            training: Whether the layer should behave in training mode or inference mode.\n",
    "        \n",
    "        Returns:\n",
    "            A triplet (z_mean, z_log_var, z) representing latent variables.\n",
    "        \"\"\"\n",
    "        inputs = self.masking(inputs)\n",
    "        x = self.layer1(inputs, training=training)\n",
    "        x = self.layer2(x, training=training)\n",
    "        x = self.layer3(x, training=training)\n",
    "        x = self.layer4(x, training=training)\n",
    "        x = self.layer5(x, training=training)\n",
    "\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package='custom', name='VAE')\n",
    "class Decoder(keras.Model):\n",
    "    def __init__(self, latent_dim=32, original_dim=61124, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.latent_dim = latent_dim\n",
    "        self.original_dim = original_dim\n",
    "\n",
    "        self.layer1 = LinearBlock(256)\n",
    "        self.layer2 = LinearBlock(256)\n",
    "        self.layer3 = LinearBlock(512)\n",
    "        self.layer4 = LinearBlock(1024)\n",
    "        self.layer5 = LinearBlock(2048)\n",
    "\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")  \n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"original_dim\": self.original_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.call(keras.random.normal(input_shape))\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.layer1(inputs, training=training)\n",
    "        x = self.layer2(x, training=training)\n",
    "        x = self.layer3(x, training=training)\n",
    "        x = self.layer4(x, training=training)\n",
    "        x = self.layer5(x, training=training)\n",
    "        return self.dense_output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package='custom', name='VAE')\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "    def __init__(self,\n",
    "            original_dim=61124, \n",
    "            latent_dim=32, \n",
    "            masking_value=-1.0, \n",
    "            loss = tf.keras.losses.MeanAbsoluteError(),\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.original_dim, self.latent_dim = original_dim, latent_dim\n",
    "        self.supports_masking, self.masking_value = True, masking_value \n",
    "        self.loss = loss\n",
    "\n",
    "        self.encoder = Encoder(self.latent_dim, self.masking_value)\n",
    "        self.decoder = Decoder(self.latent_dim, self.original_dim)\n",
    "\n",
    "        self.kl_annealing = tf.Variable(1.0, trainable=False)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"original_dim\": self.original_dim,\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"masking_value\": self.masking_value,\n",
    "            \"loss\": self.loss\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self):\n",
    "        self.encoder.build((1, self.original_dim))\n",
    "        self.decoder.build((1, self.latent_dim))\n",
    "        self.call(keras.random.normal((1, self.original_dim)))\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs, training=training)\n",
    "        reconstructed = self.decoder(z, training=training)\n",
    "        kl_loss = kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        return reconstructed, kl_loss        \n",
    "\n",
    "    def train_step(self, data):\n",
    "        x = data[0]\n",
    "        y = data[1]\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed, kl_loss = self(inputs=x, training=True)\n",
    "            reconstruction_loss = self.original_dim * self.loss(y, reconstructed)\n",
    "            ELBO = reconstruction_loss + kl_loss * self.kl_annealing\n",
    "        \n",
    "        gradients = tape.gradient(ELBO, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(x, reconstructed)\n",
    "        \n",
    "        return {\"ELBO\": ELBO, \n",
    "                \"kl_loss\": kl_loss, \n",
    "                \"reconstruction_loss\": reconstruction_loss,\n",
    "                \"annealing\": self.kl_annealing}        \n",
    "\n",
    "    def test_step(self, data):\n",
    "        x = data[0]\n",
    "        y = data[1]\n",
    "        reconstructed, kl_loss = self(inputs=x, training=False)\n",
    "        reconstruction_loss = self.original_dim * self.loss(y, reconstructed)\n",
    "        ELBO = reconstruction_loss + kl_loss\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(y, reconstructed)\n",
    "\n",
    "        return {\"ELBO\": ELBO, \"kl_loss\": kl_loss, \n",
    "                \"reconstruction_loss\": reconstruction_loss,\n",
    "                \"annealing\": self.kl_annealing}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vae(\n",
    "        original_dim=61124, \n",
    "        latent_dim=64, \n",
    "        masking_value=-1., \n",
    "        loss=tf.keras.losses.MeanAbsoluteError()\n",
    "):\n",
    "    model = VariationalAutoEncoder(original_dim, latent_dim, masking_value, loss=loss)\n",
    "    model.build()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728877007.964385  181944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728877007.990475  181944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728877007.990592  181944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "import vae_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(f'Data/train_top_5000_genes_random.npy')\n",
    "val = np.load(f'Data/val_top_5000_genes_random.npy')\n",
    "vae_helper.train_model(train, val, original_dim=5000, num_masked_genes=0, latent_dim=64, batch_size=128, num_epochs=200,\n",
    "                    kl_annealing_cycle_length=200, kl_annealing_ratio=0.8, by='random', mask_by=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(f'Data/train_top_5000_genes_kl.npy')\n",
    "val = np.load(f'Data/val_top_5000_genes_kl.npy')\n",
    "vae_helper.train_model(train, val, original_dim=5000, num_masked_genes=0, latent_dim=64, batch_size=128, num_epochs=200,\n",
    "                    kl_annealing_cycle_length=200, kl_annealing_ratio=0.8, by='random', mask_by=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(f'Data/train_top_5000_genes_frequency.npy')\n",
    "val = np.load(f'Data/val_top_5000_genes_frequency.npy')\n",
    "vae_helper.train_model(train, val, original_dim=5000, num_masked_genes=0, latent_dim=64, batch_size=128, num_epochs=200,\n",
    "                    kl_annealing_cycle_length=200, kl_annealing_ratio=0.8, by='frequency', mask_by=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(f'Data/train_scaled.npy')\n",
    "val = np.load(f'Data/val_scaled.npy')\n",
    "vae_helper.train_model(train, val, original_dim=60660, num_masked_genes=5000, latent_dim=64, batch_size=128, num_epochs=200,\n",
    "                    kl_annealing_cycle_length=200, kl_annealing_ratio=0.8, by='all', mask_by='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_helper.train_model(train, val, original_dim=60660, num_masked_genes=5000, latent_dim=64, batch_size=128, num_epochs=200,\n",
    "                    kl_annealing_cycle_length=200, kl_annealing_ratio=0.8, by='all', mask_by='kl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_helper.train_model(train, val, original_dim=60660, num_masked_genes=5000, latent_dim=64, batch_size=128, num_epochs=200,\n",
    "                    kl_annealing_cycle_length=200, kl_annealing_ratio=0.8, by='all', mask_by='frequency')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cits5017-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
